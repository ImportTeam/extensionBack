"""HTTP Fast Path â†’ Playwright fallback ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜.

ì±…ì„:
- HTTP Fast Path ì‹œë„
- Playwright í´ë°± ê´€ë¦¬
- íƒ€ì„ì•„ì›ƒ/ì˜ˆì‚° ê´€ë¦¬
- ì—ëŸ¬ í•¸ë“¤ë§
"""

from __future__ import annotations

import asyncio
import re
from typing import Any, Dict, Optional, cast

from src.core.config import settings
from src.core.exceptions import CrawlerException, ProductNotFoundException
from src.core.logging import logger
from src.utils.text_utils import clean_product_name, build_cache_key, normalize_for_search_query
from src.utils.normalization.normalize import normalize_search_query

from src.crawlers.boundary import (
    TimeoutManager,
    FastPathNoResults,
    FastPathProductFetchFailed,
)
from src.crawlers.playwright.search import search_product
from src.crawlers.playwright.detail import get_product_lowest_price


def is_broad_query(product_name: str) -> bool:
    """ğŸ”´ Broad query ê°ì§€ (timeout í™•ì¥ í•„ìš”)
    
    ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ë©´ ë‹¤ë‚˜ì™€ê°€ ë§ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê³ ,
    í˜ì´ì§€ ë¡œë”©/ë Œë”ë§ì´ ì˜¤ë˜ ê±¸ë¦¼ â†’ timeout í™•ì¥ í•„ìš”
    
    Broad query íŒì • ê¸°ì¤€:
    - 3ì ì´í•˜ (ì˜ˆ: "ê°¤ëŸ­ì‹œ ë²„ì¦ˆ" â†’ ê²€ìƒ‰ ê²°ê³¼ 5000+ê°œ)
    - ìˆ«ì/ëª¨ë¸ëª… ì—†ìŒ (ì˜ˆ: "ë§¥ë¶" vs "ë§¥ë¶ ì—ì–´ 15")
    - í•œê¸€-ì˜ë¬¸ ì„ì„ ì—†ìŒ (ì˜ˆ: "MacBook Air" â†’ êµ¬ì²´ì )
    """
    cleaned = clean_product_name(product_name).strip()
    
    # ğŸ” íŒì • ë¡œì§
    # 1) ë„ˆë¬´ ì§§ìŒ (3ì ì´í•˜) + ìˆ«ì ì—†ìŒ
    if len(cleaned) <= 3 and not re.search(r'\d', cleaned):
        logger.warning(f"[Broad] Detected: '{product_name}' is too short + no model number")
        return True
    
    # 2) ëª…ë°±íˆ genericí•œ í‚¤ì›Œë“œ (ë‹¨ì¼ ë¸Œëœë“œëª…ë§Œ)
    generic_keywords = {
        "ê°¤ëŸ­ì‹œ", "ì•„ì´í°", "ë§¥ë¶", "ë§¥", "ê°¤ëŸ­ì‹œë¶", "ê·¸ë¨", 
        "ê°¤ëŸ­ì‹œíƒ­", "ì•„ì´íŒ¨ë“œ", "ì—ì–´íŒŸ", "galaxy", "macbook"
    }
    if cleaned.lower() in generic_keywords:
        logger.warning(f"[Broad] Detected: '{product_name}' is generic brand name")
        return True
    
    # 3) "ë²„ì¦ˆ", "ì›Œì¹˜" ê°™ì€ ì¹´í…Œê³ ë¦¬ + ìˆ«ì ì—†ìŒ
    category_only = {"ë²„ì¦ˆ", "ì›Œì¹˜", "íƒœë¸”ë¦¿", "í°", "ë…¸íŠ¸ë¶"}
    if any(kw in cleaned for kw in category_only) and not re.search(r'\d', cleaned):
        logger.warning(f"[Broad] Detected: '{product_name}' is category + no generation")
        return True
    
    return False


async def search_lowest_price(
    crawler,
    product_name: str,
    product_code: Optional[str] = None,
) -> Optional[Dict]:
    """ë‹¤ë‚˜ì™€ì—ì„œ ìƒí’ˆ ê²€ìƒ‰ í›„ ìµœì €ê°€ ë°˜í™˜ (HTTP â†’ Playwright).
    
    ğŸ“ Note: product_nameì€ ì´ë¯¸ ì •ê·œí™”ëœ ì¿¼ë¦¬ì…ë‹ˆë‹¤.
    - PriceSearchServiceì—ì„œ normalize_search_query() ì ìš© í›„ ì „ë‹¬
    - ì—¬ê¸°ì„œëŠ” ìˆœìˆ˜ ì •ì œ(clean)ë§Œ ìˆ˜í–‰, ì¬ì •ê·œí™”ëŠ” í•˜ì§€ ì•ŠìŒ
    
    ğŸ”´ Broad Query Detection:
    - "ê°¤ëŸ­ì‹œ ë²„ì¦ˆ" ê°™ì€ ê²€ìƒ‰ì–´ â†’ timeout í™•ì¥ (5s â†’ 10s)
    - ë‹¤ë‚˜ì™€ì˜ ê³¼ë„í•œ ê²°ê³¼ ë°˜í™˜ ë°©ì§€
    """
    # ğŸ”´ ê¸°ê°€ì°¨ë“œ ìˆ˜ì •: ì˜ˆì‚° ë¶„ë¦¬ (HTTP 10s, Playwright 15s)
    # ì´ì œ settingsì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•Šê³  ëª…ì‹œì ìœ¼ë¡œ í• ë‹¹
    total_budget_ms = 25000 
    
    timeout_mgr = TimeoutManager(total_budget_ms)
    cb = crawler._get_circuit_breaker()

    normalized_search_term = normalize_for_search_query(product_name)
    cleaned_name = build_cache_key(normalized_search_term)  # ìˆœìˆ˜ ì •ì œë§Œ
    logger.info(f"[CRAWL] Starting search: {product_name} (HTTP: 10s, PW: 15s)")

    # ğŸ”´ ê¸°ê°€ì°¨ë“œ ìˆ˜ì •: ê²€ìƒ‰ í›„ë³´ë¥¼ ë¯¸ë¦¬ ìƒì„±í•˜ì—¬ HTTPì™€ Playwrightì—ì„œ ê³µìœ  (ì¤‘ë³µ ë¶„ì„ ë°©ì§€)
    from src.utils.search.search_optimizer import DanawaSearchHelper
    helper = DanawaSearchHelper()
    candidates = helper.generate_search_candidates(product_name)
    logger.info(f"[CRAWL] Using search candidates: {candidates}")

    # ì‹¤ì œ í¬ë¡¤ë§(ê²€ìƒ‰/ê²€ì¦)ì— ì‚¬ìš©í•  ëŒ€í‘œ ì¿¼ë¦¬.
    # ì—°ë„(2025) ê°™ì€ í† í°ì´ ë‹¤ë‚˜ì™€ ìƒí’ˆëª…ì— ì—†ì„ ìˆ˜ ìˆì–´,
    # í›„ë³´ 1ë²ˆ(ë³´í†µ ì—°ë„ ì œê±° ë²„ì „)ì„ primaryë¡œ ì‚¼ì•„ ì ìˆ˜í™”/ê²€ì¦ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    primary_query = candidates[0] if candidates else product_name

    page = None
    try:
        # 0) Fast Path (HTTP) - pcodeê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìˆ˜í–‰
        if not product_code:
            try:
                if not cb.is_open():
                    # ğŸ”´ ê¸°ê°€ì°¨ë“œ ì„¤ê³„: ë„ˆë¬´ ì§§ê±°ë‚˜ ë²”ìš©ì ì¸ ì¿¼ë¦¬ëŠ” HTTP FastPath ìŠ¤í‚µ
                    if is_broad_query(product_name) or product_name.count(" ") < 1:
                        logger.warning(f"[HTTP-FASTPATH] â© Skipping HTTP for broad/short query: '{product_name}'")
                    else:
                        logger.info(f"[HTTP-FASTPATH] Phase 1 - Attempting curl-based HTTP search (timeout: 12s)")
                        
                        # HTTP í˜ì´ì¦ˆ ì‹œì‘
                        timeout_mgr.start_phase()
                        fast = await asyncio.wait_for(
                            crawler._http.search_lowest_price(
                                query=primary_query,
                                candidates=candidates,
                                total_timeout_ms=12000, # 12s (ë‹¨ìˆœí•˜ê²Œ ì¶©ë¶„íˆ í™•ë³´)
                            ),
                            timeout=14.0, # ì—¬ìœ ë¶„ í¬í•¨
                        )
                        if fast:
                            logger.info(f"[HTTP-FASTPATH] âœ… Phase 1 SUCCESS (elapsed: {timeout_mgr.phase_elapsed_ms}ms)")
                            cb.record_success()
                            return cast(dict[str, Any] | None, fast)
                        logger.warning(f"[HTTP-FASTPATH] âš ï¸  Phase 1 RETURNED NONE")
                        cb.record_failure()
                else:
                    remaining_time = cb.get_remaining_open_time()
                    logger.warning(
                        f"[HTTP-FASTPATH] âš ï¸  Circuit breaker OPEN ({remaining_time:.1f}s remaining) - "
                        f"Skipping HTTP, going straight to Playwright"
                    )
                    cb.record_failure()
            except FastPathNoResults:
                logger.info("[HTTP-FASTPATH] âœ… Phase 1 - No products found confirmation (skipping Playwright)")
                raise ProductNotFoundException(f"No products found for: {product_name}")
            except FastPathProductFetchFailed as e:
                logger.warning(
                    f"[HTTP-FASTPATH] âš ï¸  Phase 1 PARTIAL - Got pcode={e.pcode} but product detail failed "
                    f"({e.reason}); will fetch detail via Playwright"
                )
                cb.record_failure()
                product_code = e.pcode
            except Exception as e:
                logger.warning(
                    f"[HTTP-FASTPATH] âŒ Phase 1 EXCEPTION: {type(e).__name__} - {repr(e)[:100]} | "
                    f"Falling back to Playwright"
                )
                cb.record_failure()

        # PlaywrightëŠ” ì˜ˆì‚° ë‚´ì—ì„œë§Œ ìˆ˜í–‰
        # ğŸ”´ ê¸°ê°€ì°¨ë“œ ìˆ˜ì •: Playwright í˜ì´ì¦ˆ ì‹œì‘ (ë…ë¦½ ì˜ˆì‚° 15s)
        timeout_mgr.start_phase()
        logger.info(f"[PLAYWRIGHT] Phase 2 - Fallback to Playwright (Budget: 15s)")

        # 1ë‹¨ê³„: ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì°¾ê¸°
        # ğŸ’¡ ê¸°ê°€ì°¨ë“œ ìˆ˜ì •: product_codeê°€ ìˆì–´ë„ ë‚˜ì¤‘ì— ì‹¤íŒ¨í•˜ë©´ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡° ë³€ê²½
        async def _get_pcode_via_search():
            # ì´ì „ 8ì´ˆëŠ” ë‹¤ë‚˜ì™€ ê²€ìƒ‰ í˜ì´ì§€ì— ë„ˆë¬´ ì§§ì•„ goto timeout(6s)ë¡œ ì‹¤íŒ¨ê°€ ì¦ì•˜ìŠµë‹ˆë‹¤.
            # Playwright ì „ì²´ ì˜ˆì‚°(15s) ë‚´ì—ì„œ ê²€ìƒ‰ì— ë” ë§ì€ ì‹œê°„ì„ ë°°ì •í•©ë‹ˆë‹¤.
            remaining_pw_s = timeout_mgr.phase_remaining_ms_playwright / 1000.0
            # ê²€ìƒ‰ì€ ìµœì†Œ 12ì´ˆ, ìµœëŒ€ 14ì´ˆê¹Œì§€ (ë‚¨ì€ ì˜ˆì‚° ë‚´)
            playwright_search_timeout = max(12.0, min(14.0, remaining_pw_s))
            sem_timeout = 10.0
            acquired = await crawler._acquire_browser_semaphore_with_timeout(sem_timeout)
            if not acquired:
                raise ProductNotFoundException(f"Concurrency busy for: {product_name}")
            try:
                logger.debug(f"[PLAYWRIGHT] Phase 2-A - Launching browser search (timeout: {playwright_search_timeout}s)")
                pcode = await asyncio.wait_for(
                    search_product(
                        crawler._create_page,
                        crawler.search_url,
                        primary_query,
                        overall_timeout_s=playwright_search_timeout,
                        candidates=candidates, # ğŸ”´ ê³µìœ ëœ í›„ë³´ ì‚¬ìš©
                    ),
                    timeout=playwright_search_timeout + 2.0,
                )
                if pcode:
                    logger.info(f"[PLAYWRIGHT] Phase 2-A âœ… Found product pcode: {pcode}")
                    cb.metrics.record_playwright_hit()
                return pcode
            except asyncio.TimeoutError:
                logger.error(f"[PLAYWRIGHT] Phase 2-A âŒ Search timeout")
                cb.metrics.record_playwright_failure()
                return None
            finally:
                crawler._release_browser_semaphore()

        if not product_code:
            product_code = await _get_pcode_via_search()

        if not product_code:
            raise ProductNotFoundException(f"No products found for: {product_name}")

        # 2ë‹¨ê³„: ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ìµœì €ê°€ ì¶”ì¶œ
        async def _fetch_detail(pcode: str) -> Optional[dict]:
            await crawler._rate_limit()
            remaining_pw_s = timeout_mgr.phase_remaining_ms_playwright / 1000.0
            if remaining_pw_s < 2.0:
                return None
            
            playwright_detail_timeout = min(10.0, remaining_pw_s)
            sem_timeout = playwright_detail_timeout + 2.0
            acquired = await crawler._acquire_browser_semaphore_with_timeout(sem_timeout)
            if not acquired:
                return None
            try:
                page = await crawler._create_page()
                try:
                    page.set_default_timeout(10000)
                except Exception:
                    pass
                logger.debug(f"[PLAYWRIGHT] Phase 2-B - Fetching details for pcode={pcode}")
                return await asyncio.wait_for(
                    get_product_lowest_price(page, crawler.product_url, pcode, cleaned_name),
                    timeout=playwright_detail_timeout + 2.0,
                )
            except Exception as e:
                logger.error(f"[PLAYWRIGHT] Phase 2-B âŒ Detail error for {pcode}: {e}")
                return None
            finally:
                crawler._release_browser_semaphore()

        result = await _fetch_detail(product_code)
        
        # ğŸ’¡ ê¸°ê°€ì°¨ë“œ ìˆ˜ì •: ë§Œì•½ ì œê³µëœ pcodeê°€ mismatch ë“±ìœ¼ë¡œ ì‹¤íŒ¨í–ˆë‹¤ë©´, ê²€ìƒ‰ì„ í†µí•´ ë‹¤ì‹œ ì‹œë„
        if not result and product_name:
            logger.warning(f"[PLAYWRIGHT] Provided pcode {product_code} failed. Retrying via search...")
            new_pcode = await _get_pcode_via_search()
            if new_pcode and new_pcode != product_code:
                result = await _fetch_detail(new_pcode)

        if not result:
            raise ProductNotFoundException(f"No price information for: {product_name}")

        logger.info(f"Found product: {result['product_name']} - {result['lowest_price']}ì›")
        return result

    except ProductNotFoundException:
        raise
    except Exception as e:
        logger.error(f"Crawling error for '{product_name}': {e}")
        raise CrawlerException(f"Crawling failed: {e}")
    finally:
        if page:
            try:
                await page.close()
            except Exception:
                pass
