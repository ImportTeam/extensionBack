"""HTTP Fast Path â†’ Playwright fallback ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜.

ì±…ì„:
- HTTP Fast Path ì‹œë„
- Playwright í´ë°± ê´€ë¦¬
- íƒ€ì„ì•„ì›ƒ/ì˜ˆì‚° ê´€ë¦¬
- ì—ëŸ¬ í•¸ë“¤ë§
"""

from __future__ import annotations

import asyncio
import re
from typing import Dict, Optional

from src.core.config import settings
from src.core.exceptions import CrawlerException, ProductNotFoundException
from src.core.logging import logger
from src.utils.text import clean_product_name, normalize_search_query

from src.crawlers.danawa.boundary import (
    TimeoutManager,
    FastPathNoResults,
    FastPathProductFetchFailed,
)
from src.crawlers.danawa.playwright.search import search_product
from src.crawlers.danawa.playwright.detail import get_product_lowest_price


def is_broad_query(product_name: str) -> bool:
    """ğŸ”´ Broad query ê°ì§€ (timeout í™•ì¥ í•„ìš”)
    
    ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ë©´ ë‹¤ë‚˜ì™€ê°€ ë§ì€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê³ ,
    í˜ì´ì§€ ë¡œë”©/ë Œë”ë§ì´ ì˜¤ë˜ ê±¸ë¦¼ â†’ timeout í™•ì¥ í•„ìš”
    
    Broad query íŒì • ê¸°ì¤€:
    - 3ì ì´í•˜ (ì˜ˆ: "ê°¤ëŸ­ì‹œ ë²„ì¦ˆ" â†’ ê²€ìƒ‰ ê²°ê³¼ 5000+ê°œ)
    - ìˆ«ì/ëª¨ë¸ëª… ì—†ìŒ (ì˜ˆ: "ë§¥ë¶" vs "ë§¥ë¶ ì—ì–´ 15")
    - í•œê¸€-ì˜ë¬¸ ì„ì„ ì—†ìŒ (ì˜ˆ: "MacBook Air" â†’ êµ¬ì²´ì )
    """
    cleaned = clean_product_name(product_name).strip()
    
    # ğŸ” íŒì • ë¡œì§
    # 1) ë„ˆë¬´ ì§§ìŒ (3ì ì´í•˜) + ìˆ«ì ì—†ìŒ
    if len(cleaned) <= 3 and not re.search(r'\d', cleaned):
        logger.warning(f"[Broad] Detected: '{product_name}' is too short + no model number")
        return True
    
    # 2) ëª…ë°±íˆ genericí•œ í‚¤ì›Œë“œ (ë‹¨ì¼ ë¸Œëœë“œëª…ë§Œ)
    generic_keywords = {
        "ê°¤ëŸ­ì‹œ", "ì•„ì´í°", "ë§¥ë¶", "ë§¥", "ê°¤ëŸ­ì‹œë¶", "ê·¸ë¨", 
        "ê°¤ëŸ­ì‹œíƒ­", "ì•„ì´íŒ¨ë“œ", "ì—ì–´íŒŸ", "galaxy", "macbook"
    }
    if cleaned.lower() in generic_keywords:
        logger.warning(f"[Broad] Detected: '{product_name}' is generic brand name")
        return True
    
    # 3) "ë²„ì¦ˆ", "ì›Œì¹˜" ê°™ì€ ì¹´í…Œê³ ë¦¬ + ìˆ«ì ì—†ìŒ
    category_only = {"ë²„ì¦ˆ", "ì›Œì¹˜", "íƒœë¸”ë¦¿", "í°", "ë…¸íŠ¸ë¶"}
    if any(kw in cleaned for kw in category_only) and not re.search(r'\d', cleaned):
        logger.warning(f"[Broad] Detected: '{product_name}' is category + no generation")
        return True
    
    return False


async def search_lowest_price(
    crawler,
    product_name: str,
    product_code: Optional[str] = None,
) -> Optional[Dict]:
    """ë‹¤ë‚˜ì™€ì—ì„œ ìƒí’ˆ ê²€ìƒ‰ í›„ ìµœì €ê°€ ë°˜í™˜ (HTTP â†’ Playwright).
    
    ğŸ“ Note: product_nameì€ ì´ë¯¸ ì •ê·œí™”ëœ ì¿¼ë¦¬ì…ë‹ˆë‹¤.
    - PriceSearchServiceì—ì„œ normalize_search_query() ì ìš© í›„ ì „ë‹¬
    - ì—¬ê¸°ì„œëŠ” ìˆœìˆ˜ ì •ì œ(clean)ë§Œ ìˆ˜í–‰, ì¬ì •ê·œí™”ëŠ” í•˜ì§€ ì•ŠìŒ
    
    ğŸ”´ Broad Query Detection:
    - "ê°¤ëŸ­ì‹œ ë²„ì¦ˆ" ê°™ì€ ê²€ìƒ‰ì–´ â†’ timeout í™•ì¥ (5s â†’ 10s)
    - ë‹¤ë‚˜ì™€ì˜ ê³¼ë„í•œ ê²°ê³¼ ë°˜í™˜ ë°©ì§€
    """
    total_budget_ms = int(getattr(settings, "crawler_total_budget_ms", 4000))
    
    # ğŸ”´ Broad query ê°ì§€ â†’ timeout 50% í™•ì¥
    if is_broad_query(product_name):
        original_budget = total_budget_ms
        total_budget_ms = int(total_budget_ms * 1.5)  # 4000ms â†’ 6000ms
        logger.warning(
            f"[BROAD-QUERY] Detected broad query '{product_name}' "
            f"â†’ extending timeout {original_budget}ms â†’ {total_budget_ms}ms"
        )
    
    timeout_mgr = TimeoutManager(total_budget_ms)
    cb = crawler._get_circuit_breaker()

    cleaned_name = clean_product_name(product_name)  # ìˆœìˆ˜ ì •ì œë§Œ
    logger.info(f"[CRAWL] Starting search with normalized query: {product_name} (budget: {total_budget_ms}ms)")

    page = None
    try:
        # 0) Fast Path (HTTP) - pcodeê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìˆ˜í–‰
        if not product_code:
            try:
                if not cb.is_open():
                    logger.info(
                        f"[HTTP-FASTPATH] Phase 1 - Attempting curl-based HTTP search "
                        f"(timeout: {timeout_mgr.budget.http_budget_ms}ms)"
                    )
                    from src.utils.search import DanawaSearchHelper

                    helper = DanawaSearchHelper()
                    # âœ… product_nameì€ ì´ë¯¸ ì •ê·œí™”ë˜ì—ˆìœ¼ë¯€ë¡œ, ì¶”ê°€ ë³€í˜•ë§Œ ìƒì„±
                    candidates = helper.generate_search_candidates(product_name)
                    
                    fast = await asyncio.wait_for(
                        crawler._http.search_lowest_price(
                            query=product_name,
                            candidates=candidates,
                            total_timeout_ms=timeout_mgr.budget.http_budget_ms,
                        ),
                        timeout=timeout_mgr.budget.http_budget_s + 2.0,
                    )
                    if fast:
                        elapsed_ms = timeout_mgr.elapsed_ms
                        logger.info(
                            f"[HTTP-FASTPATH] âœ… Phase 1 SUCCESS - Found price via curl "
                            f"(result: {fast.get('lowest_price', 0)}ì› from {fast.get('mall', '?')} | "
                            f"elapsed: {elapsed_ms}ms)"
                        )
                        cb.record_success()
                        return fast
                    logger.warning(f"[HTTP-FASTPATH] âš ï¸  Phase 1 RETURNED NONE - Will fallback to Playwright")
                    cb.record_failure()
                else:
                    remaining_time = cb.get_remaining_open_time()
                    logger.warning(
                        f"[HTTP-FASTPATH] âš ï¸  Circuit breaker OPEN ({remaining_time:.1f}s remaining) - "
                        f"Skipping HTTP, going straight to Playwright"
                    )
                    cb.record_failure()
            except FastPathNoResults:
                logger.info("[HTTP-FASTPATH] âœ… Phase 1 - No products found confirmation (skipping Playwright)")
                raise ProductNotFoundException(f"No products found for: {product_name}")
            except FastPathProductFetchFailed as e:
                logger.warning(
                    f"[HTTP-FASTPATH] âš ï¸  Phase 1 PARTIAL - Got pcode={e.pcode} but product detail failed "
                    f"({e.reason}); will fetch detail via Playwright"
                )
                cb.record_failure()
                product_code = e.pcode
            except Exception as e:
                logger.warning(
                    f"[HTTP-FASTPATH] âŒ Phase 1 EXCEPTION: {type(e).__name__} - {repr(e)[:100]} | "
                    f"Falling back to Playwright"
                )
                cb.record_failure()

        # PlaywrightëŠ” ì˜ˆì‚° ë‚´ì—ì„œë§Œ ìˆ˜í–‰
        remaining_s = timeout_mgr.remaining_s
        logger.info(f"[PLAYWRIGHT] Phase 2 - Fallback to Playwright browser mode (remaining_budget: {remaining_s:.2f}s)")

        if remaining_s < 0.8:
            raise asyncio.TimeoutError("crawl_budget_exhausted_before_playwright")

        # 1ë‹¨ê³„: ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ì°¾ê¸° (ì´ë¯¸ ì½”ë“œê°€ ì£¼ì–´ì§€ë©´ ìŠ¤í‚µ)
        if not product_code:
            if not timeout_mgr.has_minimum_for_playwright_search():
                raise asyncio.TimeoutError("insufficient_budget_for_playwright_search_and_detail")

            playwright_search_timeout = timeout_mgr.get_playwright_search_timeout()
            sem_timeout = min(timeout_mgr.remaining_s, playwright_search_timeout + 1.0)
            acquired = await crawler._acquire_browser_semaphore_with_timeout(sem_timeout)
            if not acquired:
                raise ProductNotFoundException(f"Concurrency busy for: {product_name}")
            try:
                logger.debug(f"[PLAYWRIGHT] Phase 2-A - Launching browser search (timeout: {playwright_search_timeout:.1f}s)")
                product_code = await asyncio.wait_for(
                    search_product(
                        crawler._create_page,
                        crawler.search_url,
                        product_name,
                        overall_timeout_s=playwright_search_timeout,
                    ),
                    timeout=playwright_search_timeout,
                )
                logger.info(f"[PLAYWRIGHT] Phase 2-A âœ… Found product pcode: {product_code}")
                cb.metrics.record_playwright_hit()
            except asyncio.TimeoutError:
                logger.error(f"[PLAYWRIGHT] Phase 2-A âŒ Search timeout after {playwright_search_timeout}s")
                cb.metrics.record_playwright_failure()
                raise
            finally:
                crawler._release_browser_semaphore()

        if not product_code:
            raise ProductNotFoundException(f"No products found for: {product_name}")

        # 2ë‹¨ê³„: ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ìµœì €ê°€ ì¶”ì¶œ
        await crawler._rate_limit()
        remaining_s = timeout_mgr.remaining_s
        if remaining_s < 1.0:
            raise asyncio.TimeoutError("insufficient_budget_for_playwright_detail")
        
        playwright_detail_timeout = timeout_mgr.get_playwright_detail_timeout()
        sem_timeout = min(timeout_mgr.remaining_s, playwright_detail_timeout + 1.0)
        acquired = await crawler._acquire_browser_semaphore_with_timeout(sem_timeout)
        if not acquired:
            raise CrawlerException(f"Playwright concurrency busy for: {product_name}")
        try:
            page = await crawler._create_page()
            # Playwright ë‚´ë¶€ selector timeoutì€ ì¶©ë¶„íˆ í™•ë³´
            try:
                page.set_default_timeout(8000)
            except Exception:
                pass
            logger.debug(f"[PLAYWRIGHT] Phase 2-B - Fetching product details (timeout: {playwright_detail_timeout:.1f}s)")
            result = await asyncio.wait_for(
                get_product_lowest_price(page, crawler.product_url, product_code, cleaned_name),
                timeout=playwright_detail_timeout,
            )
            if result:
                elapsed_ms = timeout_mgr.elapsed_ms
                logger.info(
                    f"[PLAYWRIGHT] Phase 2-B âœ… SUCCESS - Got price from Playwright "
                    f"(result: {result.get('lowest_price', 0)}ì› from {result.get('mall', '?')} | "
                    f"elapsed: {elapsed_ms}ms)"
                )
                cb.metrics.record_playwright_hit()
            else:
                logger.error(f"[PLAYWRIGHT] Phase 2-B âŒ No price data returned")
                cb.metrics.record_playwright_failure()
        except asyncio.TimeoutError:
            logger.error(f"[PLAYWRIGHT] Phase 2-B âŒ Detail page timeout after {playwright_detail_timeout}s")
            cb.metrics.record_playwright_failure()
            raise
        except Exception as e:
            logger.error(
                f"[PLAYWRIGHT] Phase 2-B âŒ Detail page error: {type(e).__name__}: {repr(e)[:100]}"
            )
            cb.metrics.record_playwright_failure()
            raise
        finally:
            crawler._release_browser_semaphore()

        if not result:
            raise ProductNotFoundException(f"No price information for: {product_name}")

        logger.info(f"Found product: {result['product_name']} - {result['lowest_price']}ì›")
        return result

    except ProductNotFoundException:
        raise
    except Exception as e:
        logger.error(f"Crawling error for '{product_name}': {e}")
        raise CrawlerException(f"Crawling failed: {e}")
    finally:
        if page:
            try:
                await page.close()
            except Exception:
                pass
